# Import necessary libraries
import os
import re
import requests
from datetime import datetime, timezone, timedelta
from pymongo import MongoClient
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv


from .gpt_answer import ask_gpt_answer_only
from .gpt_answer import process_user_query
from .gpt_answer import initialize_database
import vector

# Load environment variables (requires a `.env` file with proper keys)
load_dotenv()

# Azure OpenAI and other service credentials
azure_oai_endpoint = os.getenv("AZURE_OAI_ENDPOINT")
azure_oai_key = os.getenv("AZURE_OAI_KEY")
azure_oai_deployment = os.getenv("AZURE_OAI_DEPLOYMENT")
azure_textembedding_endpoint = os.getenv("AZURE_TEXTEMBEDDING_ENDPOINT")
azure_textembedding_key = os.getenv("AZURE_TEXTEMBEDDING_KEY")
azure_search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT")
azure_search_key = os.getenv("AZURE_SEARCH_KEY")
azure_services_endpoint = os.getenv("AZURE_SERVICES_ENDPOINT")
azure_services_key = os.getenv("AZURE_SERVICES_KEY")
cosmos_endpoint = os.getenv("COSMOSDB_NOSQL_ENDPOINT")
cosmos_key = os.getenv("COSMOSDB_NOSQL_KEY")
mongodb_uri = os.getenv("MONGODB_URI")

client = MongoClient(mongodb_uri)
db = client["6a013"]
collection = db["chat_logs"]

# ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ í‰ê°€í•˜ê³  ì í•©í•œ ì¹´í…Œê³ ë¦¬ì— ì¶”ê°€
def classify_and_append_query(query):
    categories_collection= initialize_database("6a055","category_classified")
    query_embedding = vector.vectorize(query)

    documents = list(categories_collection.find({}))

    best_match = None
    best_score = -1

    for document in documents:
        subcategory_embedding = document["subcategoryvector"]
        similarity = cosine_similarity([query_embedding], [subcategory_embedding])[0][0]

        if similarity > best_score:
            best_score = similarity
            best_match = document

    # âœ… ìœ ì‚¬ë„ê°€ ì¶©ë¶„íˆ ë†’ì„ ë•Œë§Œ ì €ì¥
    threshold = 0.44 # ë„ˆê°€ ì›í•˜ëŠ” ê¸°ì¤€ì— ë§ê²Œ ì¡°ì ˆ ê°€ëŠ¥

    if best_match and best_score >= threshold:
        categories_collection.update_one(
            {"_id": best_match["_id"]},
            {"$push": {"questions": query}}
        )
        category_name = best_match["category"]
        subcategory_name = best_match["subcategory"]
        print(f"'{query}' ì§ˆë¬¸ì´ '{category_name}'ì˜ '{subcategory_name}' í•˜ìœ„ í•­ëª©ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.(ìœ ì‚¬ë„: {best_score:.2f})")
    else:
        category_name, subcategory_name = "ê¸°íƒ€", "ê¸°íƒ€"
        print(f"'{query}'ëŠ” ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ìœ ì‚¬ë„: {best_score:.2f})")

    return category_name, subcategory_name,query_embedding

# í•¨ìˆ˜ ì„ ì–¸
def ask_gpt_rag(user_id, query):
    # endpoint, headers, body ë³€ìˆ˜ ë§Œë“¤ê¸°
    endpoint = azure_oai_endpoint
    headers = {
        "Content-Type": "application/json",
        "api-key": azure_oai_key
    }

    body = {
        "messages": [
            {
                "role": "user",
                "content": query
            }
        ],
        "temperature": 0.7,
        "top_p": 0.95,
        "max_tokens": 800,
        "data_sources": [
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": azure_search_endpoint,
                    "index_name": "vector-yj-test-2",
                    "semantic_configuration": "vector-yj-test-2-semantic-configuration",
                    "query_type": "semantic",
                    "fields_mapping": {},
                    "in_scope": True,
                    "role_information": "",
                    "filter": None,
                    "strictness": 1,
                    "top_n_documents": 1,
                    "authentication": {
                        "type": "api_key",
                        "key": azure_search_key
                    },
                    "key": azure_search_key,
                    "indexName": "vector-yj-test-2"
                }
            }
        ]
    }
    ta_credential = AzureKeyCredential(azure_services_key)
    client = TextAnalyticsClient(
        endpoint=azure_services_endpoint,
        credential=ta_credential)
    # ì‘ë‹µ ìš”ì²­
    response = requests.post(endpoint, headers=headers, json=body)

    # 200 OK
    if response.status_code == 200:
        response_json = response.json()
        answer = response_json["choices"][0]["message"]["content"]
        if ("The requested information is not" in answer
                or "ìš”ì²­ëœ ì •ë³´" in answer
                or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in answer):
            print("answer:",answer)
            rag_log=ask_gpt_answer_only(user_id,query)
            return rag_log, ""
        unix_time = response_json['created']
        KST = timezone(timedelta(hours=9))
        dt = datetime.fromtimestamp(unix_time, tz=KST)
        poller = client.begin_abstract_summary([answer], sentence_count=1, language="ko")
        summary = ""
        abstract_summary_results = poller.result()
        for result in abstract_summary_results:
            if result.kind == "AbstractiveSummarization":
                for content in result.summaries:
                    summary += content.text
            elif result.is_error is True:
                print("...Is an error with code '{}' and message '{}'".format(result.error.code, result.error.message))

        citations = response_json["choices"][0]["message"]["context"]["citations"]
        category_name,subcategory_name,vector=classify_and_append_query(query)
        timestamps = []
        for citation in citations:
            content = citation.get("content", "")
            # ì •ê·œì‹ìœ¼ë¡œ timestamp íŒ¨í„´ ì¶”ì¶œ
            found = re.findall(r'"timestamp":\s*"([^"]+)"', content)
            timestamps.extend(found)

        rag_log = {
            "user_id": user_id,
            "datetime": dt.isoformat(),  # ISO 8601 ë¬¸ìì—´ë¡œ ì €ì¥ (MongoDBì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìë™ ì¸ì‹)
            "query": query,
            "answer": answer,
            "summary": summary,
            "timestamps": timestamps,
            "category": {
                "category_name": category_name,
                "subcategory_name": subcategory_name
            },
            "query_vectorized": vector
        }

        print(response_json)

        view_points = ""
        # print("ğŸ“Œ ê´€ë ¨ íƒ€ì„ìŠ¤íƒ¬í”„:")

        for ts in timestamps:
            print("-", ts)
            view_points += f"{ts}\n"

        return rag_log,view_points
    else:
        return response.status_code, ""

def convert_timestamp_to_seconds(timestamp):
    """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ (00:00:00 í˜•ì‹)"""
    parts = timestamp.split(':')
    if len(parts) == 2:  # MM:SS í˜•ì‹
        return int(parts[0]) * 60 + int(parts[1])
    elif len(parts) == 3:  # HH:MM:SS í˜•ì‹
        return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
    return 0

def create_youtube_timestamp_link(timestamp, video_id="UwA8gp-7oeY"):
    """íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìœ íŠœë¸Œ ë§í¬ ìƒì„±"""
    # ë²”ìœ„ì—ì„œ ì²« ë²ˆì§¸ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (ì˜ˆ: "00:22:12 ~ 00:23:30" -> "00:22:12")
    match = re.search(r"(\d{2}:\d{2}:\d{2}|\d{2}:\d{2})", timestamp)
    if match:
        start_time = match.group(1)
        seconds = convert_timestamp_to_seconds(start_time)
        return f"https://www.youtube.com/watch?v={video_id}&t={seconds}s"
    return f"https://www.youtube.com/watch?v={video_id}"

def update_timestamp_buttons(view_points):
    """íƒ€ì„ìŠ¤íƒ¬í”„ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ HTML ë²„íŠ¼ìœ¼ë¡œ ë³€í™˜"""
    if not view_points:
        return ""
    # view_pointsê°€ ë¬¸ìì—´ì¼ ë•Œ ì²˜ë¦¬
    if isinstance(view_points, str):
        timestamp_list = view_points.strip().split('\n')
    # view_pointsê°€ ë¦¬ìŠ¤íŠ¸ì¼ ë•Œ ì²˜ë¦¬
    elif isinstance(view_points, list):
        timestamp_list = [str(item).strip() for item in view_points]



    # ë²„íŠ¼ ì¶œë ¥ìš© (HTMLì— í‘œì‹œ)
    buttons_html = "<div style='margin-top: 10px;'>"

    for ts in timestamp_list:
        if ts.strip():  # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
            link = create_youtube_timestamp_link(ts)
            # ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ ë²„íŠ¼ HTML ìƒì„±
            buttons_html += f"""
            <a href="{link}" target="_blank" style="
                display: inline-block;
                margin: 5px;
                padding: 8px 15px;
                background-color: #4285f4;
                color: white;
                text-decoration: none;
                border-radius: 4px;
                font-weight: bold;
                cursor: pointer;
            ">{ts}</a>
            """

    buttons_html += "</div>"
    return buttons_html

# ì¿¼ë¦¬ë¥¼ ë°›ìœ¼ë©´ citationì„ URLë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def gpt_ask_gpt_rag_image(query):
    endpoint = azure_oai_endpoint
    headers = {
        "Content-Type": "application/json",
        "api-key": azure_oai_key
    }

    body = {
        "data_sources": [
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": azure_search_endpoint,
                    "index_name": "pdf-indexer",
                    "semantic_configuration": "pdf-image-semantic",
                    "query_type": "semantic",
                    "fields_mapping": {},
                    "in_scope": True,
                    "role_information": "",
                    "filter": None,
                    "strictness": 3,
                    "top_n_documents": 5,
                    "authentication": {
                        "type": "api_key",
                        "key": azure_search_key
                    }
                }
            }
        ],
        "messages": [
            {
                "role": "user",
                "content": query
            }
        ],
        "temperature": 0.7,
        "top_p": 0.95,
        "max_tokens": 800,
        "stream": False,
        "frequency_penalty": 0,
        "presence_penalty": 0
    }

    response = requests.post(endpoint, headers=headers, json=body)

    if response.status_code != 200:
        print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}, {response.text}")
        return None

    data = response.json()
    citations = []

    try:
        context = data["choices"][0]["message"]["context"]
        raw_citations = context.get("citations", [])

        for citation in raw_citations:
            content = citation.get("content", "")
            urls = re.findall(r"https:\/\/[^\s]+?\.png", content)
            citations.extend(urls)

        if not citations:
            return None
        return citations

    except Exception as e:
        print("ì—ëŸ¬ ë°œìƒ:", e)
        return "None"

# ì´ë¯¸ì§€ URLì„ HTMLë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def render_image_gallery_html(urls: list[str]) -> str:
    if not urls:
        return "<p><i>ê´€ë ¨ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.</i></p>"

    gallery_html = "<div style='display: flex; flex-wrap: wrap; gap: 10px;'>"
    for url in urls:
        gallery_html += f"""
        <div style="border:1px solid #ddd; padding:4px;">
            <img src="{url}" style="width:180px; border-radius:4px;" alt="image"/>
        </div>
        """
    gallery_html += "</div>"
    return  gallery_html

def gpt_ask_gpt_rag_answer_only(user_id, query):
    urls = gpt_ask_gpt_rag_image(query)
    gallery_html = render_image_gallery_html(urls)
    chat_log, citation = ask_gpt_rag(user_id=user_id, query=query)
    answer = chat_log['answer']
    timestamp_buttons = update_timestamp_buttons(citation)
    return answer, chat_log, timestamp_buttons, gallery_html