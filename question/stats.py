from datetime import datetime, timezone, timedelta
from pymongo import MongoClient
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
from dotenv import load_dotenv
import gradio as gr
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from bson import ObjectId
import requests
import os
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans, AgglomerativeClustering
import json
import webbrowser
from io import BytesIO
import base64
from openai import AzureOpenAI
from jinja2 import Template


client = MongoClient("mongodb+srv://team9:1q2w3e4r!@6th-team9-2nd-db.global.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000")
db = client["6a013"]
collection = db["chat_logs"]

azure_oai_endpoint = os.getenv("AZURE_OAI_ENDPOINT")
azure_oai_key = os.getenv("AZURE_OAI_KEY")

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

# Review stages (label and time delta for each stage)
REVIEW_STAGES = [
    ("1ì°¨ ë³µìŠµ", timedelta(days=1)),
    ("2ì°¨ ë³µìŠµ", timedelta(days=7)),
    ("3ì°¨ ë³µìŠµ", timedelta(days=30))
]

# ì§ˆë¬¸ í†µê³„ í•¨ìˆ˜
def get_stats(user_id):
    docs = list(collection.find({"user_id": user_id}))
    now = datetime.now(timezone.utc)
    labels, due, done = [], [], []

    for i, (label, delta) in enumerate(REVIEW_STAGES):
        d, dn = 0, 0
        for doc in docs:
            dt = datetime.fromisoformat(doc["datetime"].replace("Z", "+00:00"))
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)  # âœ… íƒ€ì„ì¡´-awareë¡œ ë³´ì •
            base = dt if doc.get("review_counts", 0) == 0 else datetime.fromisoformat(doc.get("review_date", dt.isoformat()).replace("Z", "+00:00"))
            if base.tzinfo is None:
                base = base.replace(tzinfo=timezone.utc)
            rc = doc.get("review_counts", 0)
            if rc <= i and base + delta < now:
                d += 1
            elif rc > i:
                dn += 1
        labels.append(label)
        due.append(d)
        done.append(dn)

    # ê·¸ë¦¬ê¸°
    fig, ax = plt.subplots(figsize=(5, 3))
    ax.bar(labels, due, label="ë³µìŠµ í•„ìš”", color="#ff6b6b")
    ax.bar(labels, done, bottom=due, label="ë³µìŠµ ì™„ë£Œ", color="#4caf50")
    ax.set_ylabel("ì§ˆë¬¸ ìˆ˜")
    ax.set_title("ë³µìŠµ í˜„í™©")
    ax.legend()
    return fig

# ì „ì²´ í•™ìŠµì(ë³¸ì¸ ì œì™¸) ì§ˆë¬¸ í™•ì¸ í•¨ìˆ˜
def load_popular_questions(user_id):
    cursor = collection.find({"user_id": {"$ne": user_id}}, {"query": 1})
    queries = [doc["query"] for doc in cursor if "query" in doc]
    counter = Counter(queries)
    sorted_questions = counter.most_common(30)
    top_list = [f"{q} - {count}íšŒ" for q, count in sorted_questions]
    return gr.update(choices=top_list), pd.DataFrame(sorted_questions, columns=["ì§ˆë¬¸", "íšŸìˆ˜"])

### ğŸ”¹ ìµœê·¼ ì§ˆë¬¸ 20ê°œ ë¶ˆëŸ¬ì˜¤ê¸° (ë¼ë””ì˜¤ ë²„íŠ¼ìš©)
def load_recent_questions():
    db = client["6a013"]
    collection = db["chat_logs"]

    cursor = collection.find({}, {"query": 1, "timestamp": 1}) \
        .sort("timestamp", -1).limit(20)

    labels = []
    for doc in cursor:
        query = doc.get("query")
        ts = doc.get("timestamp")
        if query:
            label = f"{query} ({ts.strftime('%Y-%m-%d %H:%M')})" if ts else query
            labels.append(label)
    print("labels:",labels)
    return gr.update(choices=labels)

def load_all_questions_summary():
    cursor = collection.find({}, {"query": 1, "summary": 1, "timestamp": 1}) \
        .sort("timestamp", -1)

    rows = []
    for doc in cursor:
        ts = doc.get("timestamp")
        query = doc.get("query", "")
        summary = doc.get("summary", "")
        time_str = ts.strftime('%Y-%m-%d %H:%M') if ts else ""
        rows.append({
            "ì§ˆë¬¸": query,
            "ìš”ì•½": summary
        })

    df = pd.DataFrame(rows)
    print(df)
    return df

def cluster_questions(n_clusters=7):
    # MongoDBì—ì„œ ì§ˆë¬¸ê³¼ ë²¡í„° ë¡œë“œ
    docs = list(collection.find({"query_vectorized": {"$exists": True}}))
    queries, answers, vectors = [], [], []

    for doc in docs:
        vec = doc.get("query_vectorized")
        if vec and isinstance(vec, list) and all(isinstance(x, (int, float)) for x in vec) and len(vec) == 1536:
            vectors.append(vec)
            queries.append(doc["query"])
            answers.append(doc["answer"])

    if len(vectors) < n_clusters:
        return [], "â— í´ëŸ¬ìŠ¤í„° ìˆ˜ë³´ë‹¤ ì§ˆë¬¸ì´ ì ìŠµë‹ˆë‹¤.", {}, [], []

    X = np.array(vectors)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)

    clusters = {i: [] for i in range(n_clusters)}
    for i, label in enumerate(labels):
        clusters[label].append((queries[i], answers[i]))

    cluster_keywords = {}
    for cid, items in clusters.items():
        texts = [q for q, _ in items]
        tfidf = TfidfVectorizer(stop_words='english')
        X = tfidf.fit_transform(texts)
        scores = X.sum(axis=0)
        words = [(word, scores[0, idx]) for word, idx in tfidf.vocabulary_.items()]
        sorted_words = sorted(words, key=lambda x: x[1], reverse=True)
        cluster_keywords[cid] = [w[0] for w in sorted_words[:3]]

    keyword_choices = [", ".join(cluster_keywords[i]) for i in range(n_clusters)]
    question_map = {", ".join(cluster_keywords[i]): clusters[i] for i in range(n_clusters)}

    return keyword_choices, "âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ", question_map, queries, answers

def delete_question(selected_id, docs):
    try:
        obj_id = ObjectId(selected_id)
    except Exception as e:
        return gr.update(choices=[]), [], f"ì‚­ì œ ì˜¤ë¥˜: {e}", "", "", gr.update(visible=False)
    collection.delete_one({"_id": obj_id})
    return gr.update(choices=[]), [], "", "", "", gr.update(visible=False)




def generate_cluster_name_from_keywords(keywords):
    prompt = f"ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ ê°€ì¥ ì˜ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” í•˜ë‚˜ì˜ ê°„ë‹¨í•œ í•œêµ­ì–´ í‚¤ì›Œë“œë¡œ ìš”ì•½í•´ì¤˜. ê°€ëŠ¥í•œ í•œ ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ë¡œ ì‘ì„±í•´ì¤˜:\ní‚¤ì›Œë“œ: {', '.join(keywords)}"
    headers = {"Content-Type": "application/json", "api-key": azure_oai_key}
    body = {
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œë“¤ì„ ë°›ì•„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì˜ë¯¸ ìˆëŠ” ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. ë‹µë³€ì—ëŠ” ë”°ì˜´í‘œë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.5, "top_p": 0.9, "max_tokens": 50
    }
    try:
        response = requests.post(azure_oai_endpoint, headers=headers, json=body)
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"].strip()
    except:
        pass
    return "ë¯¸ì •_í‚¤ì›Œë“œ"





def get_vectorized_docs():
    collection = db["chat_logs_yj"]
    return list(collection.find({"query_vectorized": {"$exists": True, "$type": "array"}}))

def cluster_questions_kmeans(n_clusters):
    docs = get_vectorized_docs()
    if len(docs) < n_clusters:
        return [], f"â— ì§ˆë¬¸ ìˆ˜({len(docs)}) < í´ëŸ¬ìŠ¤í„° ìˆ˜({n_clusters})", {}

    queries, answers, vectors = [], [], []
    for doc in docs:
        vec = doc["query_vectorized"]
        if len(vec) == 1536:
            queries.append(doc["query"])
            answers.append(doc["answer"])
            vectors.append(vec)

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(np.array(vectors))

    clusters = defaultdict(list)
    for i, label in enumerate(labels):
        clusters[label].append((queries[i], answers[i]))

    question_map = {}
    for cid, items in clusters.items():
        texts = [q for q, _ in items]
        tfidf = TfidfVectorizer(stop_words='english')
        X_tfidf = tfidf.fit_transform(texts)
        scores = X_tfidf.sum(axis=0)
        words = [(word, scores[0, idx]) for word, idx in tfidf.vocabulary_.items()]
        top_keywords = [w[0] for w in sorted(words, key=lambda x: x[1], reverse=True)[:3]]
        keyword = generate_cluster_name_from_keywords(top_keywords)
        question_map[keyword] = items
    return list(question_map.keys()), "âœ… KMeans ì™„ë£Œ", question_map

def cluster_questions_similarity(threshold):
    docs = get_vectorized_docs()
    if len(docs) < 2:
        return [], "â— ìœ ì‚¬ë„ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ì§ˆë¬¸ ìˆ˜ ë¶€ì¡±", {}

    queries, answers, vectors = [], [], []
    for doc in docs:
        vec = doc["query_vectorized"]
        if len(vec) == 1536:
            queries.append(doc["query"])
            answers.append(doc["answer"])
            vectors.append(vec)

    sim_matrix = cosine_similarity(vectors)
    dist_matrix = 1 - sim_matrix

    clustering = AgglomerativeClustering(metric='precomputed', linkage='complete', distance_threshold=1 - threshold, n_clusters=None)
    labels = clustering.fit_predict(dist_matrix)

    clusters = defaultdict(list)
    for i, label in enumerate(labels):
        clusters[label].append((queries[i], answers[i]))

    question_map = {}
    for cid, items in clusters.items():
        texts = [q for q, _ in items]
        tfidf = TfidfVectorizer(stop_words='english')
        X_tfidf = tfidf.fit_transform(texts)
        scores = X_tfidf.sum(axis=0)
        words = [(word, scores[0, idx]) for word, idx in tfidf.vocabulary_.items()]
        top_keywords = [w[0] for w in sorted(words, key=lambda x: x[1], reverse=True)[:3]]
        keyword = generate_cluster_name_from_keywords(top_keywords)
        question_map[keyword] = items
    return list(question_map.keys()), "âœ… Similarity ì™„ë£Œ", question_map

def generate_kmeans_similarity_statistics():
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams.update({'axes.titlesize': 14, 'xtick.labelsize': 10, 'ytick.labelsize': 10})
    collection = db["chat_logs_yj"]
    logs = list(collection.find({}))

    def count_by_level(logs, category_field):
        basic_counter, adv_counter = Counter(), Counter()
        basic_time, adv_time = Counter(), Counter()

        for log in logs:
            level = log.get("level", "unknown")
            dt_str = log.get("datetime")
            try:
                dt = datetime.fromisoformat(dt_str.replace("Z", "+00:00"))
                hour_key = f"{dt.hour:02d}ì‹œ"
            except:
                hour_key = "unknown"

            cat = log.get(category_field)
            if cat:
                if level == "basic":
                    basic_counter[str(cat)] += 1
                    basic_time[hour_key] += 1
                elif level == "advanced":
                    adv_counter[str(cat)] += 1
                    adv_time[hour_key] += 1

        return basic_counter, adv_counter, basic_time, adv_time

    kmeans_b, kmeans_a, kmeans_bt, kmeans_at = count_by_level(logs, "category_kmeans")
    sim_b, sim_a, sim_bt, sim_at = count_by_level(logs, "category_similarity")

    def plot_grouped_bar(b_counter, a_counter, title):
        labels = sorted(set(b_counter.keys()).union(a_counter.keys()))
        b_vals = [b_counter.get(k, 0) for k in labels]
        a_vals = [a_counter.get(k, 0) for k in labels]
        x = np.arange(len(labels))
        width = 0.35

        fig, ax = plt.subplots(figsize=(7, 4))
        ax.bar(x - width/2, b_vals, width, label='Basic', color='skyblue')
        ax.bar(x + width/2, a_vals, width, label='Advanced', color='orange')
        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=30)
        ax.set_title(title)
        ax.set_ylabel("ì§ˆë¬¸ ìˆ˜")
        ax.legend()
        ax.grid(True, linestyle="--", alpha=0.5)
        return fig

    def plot_dual_line(b_time, a_time, title):
        keys = sorted(set(b_time.keys()).union(a_time.keys()), key=lambda x: int(x.replace("ì‹œ", "")) if x != "unknown" else -1)
        b_vals = [b_time.get(k, 0) for k in keys]
        a_vals = [a_time.get(k, 0) for k in keys]

        fig, ax = plt.subplots(figsize=(7, 4))
        ax.plot(keys, b_vals, label="Basic", marker='o', color='blue')
        ax.plot(keys, a_vals, label="Advanced", marker='s', color='red')
        ax.set_title(title)
        ax.set_ylabel("ì§ˆë¬¸ ìˆ˜")
        ax.set_xlabel("ì‹œê°„ëŒ€")
        ax.legend()
        ax.grid(True, linestyle="--", alpha=0.5)
        return fig

    return (
        plot_grouped_bar(kmeans_b, kmeans_a, "KMeans ì¹´í…Œê³ ë¦¬ë³„ ì§ˆë¬¸ ìˆ˜ (basic vs advanced)"),
        plot_grouped_bar(sim_b, sim_a, "Similarity ì¹´í…Œê³ ë¦¬ë³„ ì§ˆë¬¸ ìˆ˜ (basic vs advanced)"),
        plot_dual_line(kmeans_bt, kmeans_at, "ì‹œê°„ëŒ€ë³„ KMeans ì§ˆë¬¸ ìˆ˜ (basic vs advanced)"),
        plot_dual_line(sim_bt, sim_at, "ì‹œê°„ëŒ€ë³„ Similarity ì§ˆë¬¸ ìˆ˜ (basic vs advanced)")
    )

def save_chart(fig):
    buf = BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    return base64.b64encode(buf.getvalue()).decode()

def generate_line_chart(basic_data, adv_data):
    df = pd.DataFrame({"ì‹œê°„ëŒ€": sorted(set(basic_data) | set(adv_data))})
    df["Basic"] = df["ì‹œê°„ëŒ€"].map(basic_data).fillna(0)
    df["Advanced"] = df["ì‹œê°„ëŒ€"].map(adv_data).fillna(0)

    fig, ax = plt.subplots()
    ax.plot(df["ì‹œê°„ëŒ€"], df["Basic"], marker='o', linewidth=2.5, label='Basic')
    ax.plot(df["ì‹œê°„ëŒ€"], df["Advanced"], marker='o', linewidth=2.5, label='Advanced')
    ax.set_title("ì‹œê°„ëŒ€ë³„ ì§ˆë¬¸ ìˆ˜ (Basic vs Advanced)")
    ax.set_xlabel("ì‹œê°„ëŒ€")
    ax.set_ylabel("ì§ˆë¬¸ ìˆ˜")
    ax.legend()
    plt.xticks(rotation=45)
    return save_chart(fig)

def generate_grouped_bar_chart(basic_count, adv_count, title, xlabel):
    labels = sorted(set(basic_count) | set(adv_count))
    x = range(len(labels))
    basic = [basic_count.get(l, 0) for l in labels]
    adv = [adv_count.get(l, 0) for l in labels]

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.bar([i - 0.2 for i in x], basic, width=0.4, label="Basic", color="#4c72b0")
    ax.bar([i + 0.2 for i in x], adv, width=0.4, label="Advanced", color="#dd8452")
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha="right")
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel("ì§ˆë¬¸ ìˆ˜")
    ax.legend()
    return save_chart(fig)

def generate_html_report():
    collection = db["chat_logs_yj"]
    logs = list(collection.find({
        "category_kmeans": {"$exists": True},
        "category_similarity": {"$exists": True},
        "level": {"$in": ["basic", "advanced"]},
        "datetime": {"$exists": True}
    }))

    kmeans_clustered, sim_clustered = defaultdict(list), defaultdict(list)
    hour_basic, hour_adv = Counter(), Counter()
    kmeans_basic, kmeans_adv = Counter(), Counter()
    sim_basic, sim_adv = Counter(), Counter()
    gpt_logs_summary = []

    for log in logs:
        query = log["query"]
        level = log["level"]
        cat_k = log["category_kmeans"]
        cat_s = log["category_similarity"]
        time = datetime.fromisoformat(log["datetime"].replace("Z", "+00:00"))
        hour = f"{time.hour:02d}:00"

        kmeans_clustered[cat_k].append(query)
        sim_clustered[cat_s].append(query)

        if level == "basic":
            hour_basic[hour] += 1
            kmeans_basic[cat_k] += 1
            sim_basic[cat_s] += 1
        else:
            hour_adv[hour] += 1
            kmeans_adv[cat_k] += 1
            sim_adv[cat_s] += 1

        gpt_logs_summary.append({
            "level": level,
            "category_kmeans": cat_k,
            "category_similarity": cat_s,
            "datetime": log["datetime"]
        })

    charts = {
        "hour": generate_line_chart(hour_basic, hour_adv),
        "kmeans": generate_grouped_bar_chart(kmeans_basic, kmeans_adv, "KMeans ì¹´í…Œê³ ë¦¬ë³„ ì§ˆë¬¸ ìˆ˜", "ì¹´í…Œê³ ë¦¬"),
        "similarity": generate_grouped_bar_chart(sim_basic, sim_adv, "Similarity ì¹´í…Œê³ ë¦¬ë³„ ì§ˆë¬¸ ìˆ˜", "ì¹´í…Œê³ ë¦¬")
    }

    gpt_summary = json.dumps(gpt_logs_summary[:30], ensure_ascii=False, indent=2)
    client_oai = AzureOpenAI(api_key=azure_oai_key, azure_endpoint=azure_oai_endpoint, api_version="2023-07-01-preview")
    gpt_response = client_oai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": (
                    "ë„ˆëŠ” GPTutor êµìˆ˜ì ë¶„ì„ ë„êµ¬ì˜ AIì•¼. "
                    "ì•„ë˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ HTML ë³´ê³ ì„œì— ë§ëŠ” ë¶„ì„ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶œë ¥í•´ì¤˜."
                    "ê° ì¸ì‚¬ì´íŠ¸ëŠ” <div class='insight-block'>...</div> êµ¬ì¡°ë¥¼ ë”°ë¥´ë©° í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆì–´."
                    "ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆ."
                    "levelì—ì„œ basicì€ ê¸°ì´ˆë°˜, advancedëŠ” ì‹¬í™”ë°˜ì„ ì˜ë¯¸í•´"
                )
            },
            {
                "role": "user",
                "content": (
                        "ë‹¤ìŒì€ í•™ìƒ ì§ˆë¬¸ ë©”íƒ€ë°ì´í„°ì…ë‹ˆë‹¤. level, category_kmeans, category_similarity, datetimeì„ í¬í•¨í•©ë‹ˆë‹¤."
                        + gpt_summary)
            }
        ],
        temperature=0.4
    )

    feedback = gpt_response.choices[0].message.content.strip().replace("```html", "").replace("```", "")

    with open("template.html", encoding="utf-8") as f:
        template = Template(f.read())

    rendered_html = template.render(
        created=datetime.now().strftime("%Y-%m-%d"),
        charts=charts,
        kmeans_clustered=kmeans_clustered,
        sim_clustered=sim_clustered,
        feedback=feedback
    )

    with open("report.html", "w", encoding="utf-8") as f:
        f.write(rendered_html)

    return "âœ… report.html ìƒì„± ì™„ë£Œ! 'ë¦¬í¬íŠ¸ ì—´ê¸°'ë¥¼ ëˆŒëŸ¬ í™•ì¸í•´ë³´ì„¸ìš”."

def open_report_in_browser():
    path = os.path.abspath("report.html")
    webbrowser.open(f"file://{path}")
    return "ğŸŒ ë¸Œë¼ìš°ì €ì—ì„œ report.html ì—´ë ¸ìŠµë‹ˆë‹¤."






